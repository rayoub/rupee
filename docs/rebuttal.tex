% LaTeX rebuttal letter example.
% 2018 fzenke.net
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{lipsum} % to generate some filler text
\usepackage{fullpage}
\usepackage{natbib}

% import Eq and Section references from the main manuscript where needed
% \usepackage{xr}
% \externaldocument{manuscript}

% package needed for optional arguments
\usepackage{xifthen}
% define counters for reviewers and their points
\newcounter{reviewer}
\setcounter{reviewer}{0}
\newcounter{point}[reviewer]
\setcounter{point}{0}

% This refines the format of how the reviewer/point reference will appear.
\renewcommand{\thepoint}{P\,\thereviewer.\arabic{point}} 

% command declarations for reviewer points and our responses
\newcommand{\reviewersection}{\stepcounter{reviewer} \bigskip \hrule
                  \section*{Reviewer \thereviewer}}

\newenvironment{point}
   {\refstepcounter{point} \bigskip \noindent {\textbf{Reviewer~Point~\thepoint} } ---\ }
   {\par }

\newcommand{\shortpoint}[1]{\refstepcounter{point}  \bigskip \noindent 
	{\textbf{Reviewer~Point~\thepoint} } ---~#1\par }

\newenvironment{reply}
   {\medskip \noindent \begin{sf}\textbf{Reply}:\  }
   {\medskip \end{sf}}

\newcommand{\shortreply}[2][]{\medskip \noindent \begin{sf}\textbf{Reply}:\  #2
	\ifthenelse{\equal{#1}{}}{}{ \hfill \footnotesize (#1)}%
	\medskip \end{sf}}



\begin{document}

\section*{Response to the reviewers}

% Both Reviewers
We thank the reviewers for their critical assessment of our work. 
We address their concerns below. 
Where line numbers are given, they refer to line numbers in the diff. 

% Reviewer 1
\reviewersection

\subsection*{Related work}

\begin{point}
The authors should also briefly introduce the TM-score as a metric, because the authors use this metric later.
\label{pt:rw1}
\end{point}

\begin{reply}
We added the following text at lines 46-52.
\begin{quote}
However, the RMSD score does not factor in the distance between unaligned residues nor does it consider the percentage of aligned residues, that is, alignment \emph{coverage}. 
RMSD scores also have some dependence on the length of the aligned proteins. 
On the other hand, the TM-score \citep{Zhang2004} takes all residues into account and normalizes for both coverage and length of the aligned proteins. 
For this reason, TM-score is frequently used in the assessment of protein structure alignments. 
\end{quote}
\end{reply}

\begin{point}
The authors should also briefly discuss and cite other pairwise structure alignment tools like TM-align and DALI.
\label{pt:rw2}
\end{point}

\begin{reply}
We added the following text at lines 63-72. The second paragraph is part of our reply to \ref{pt:r7} and is shown here to provide further context. 
\begin{quote}
Besides CE and FATCAT, TM-align \citep{Zhang2005} and DALI \citep{Sander1995} are pairwise alignment tools also in wide use, both offering their own distinct approaches to structure alignment. 
TM-align uses a rotation matrix designed to maximize the TM-score rather than minimizing the RMSD along with dynamic programming to find the best full-length alignment. 
DALI compares intramolecular distance matrices between two proteins to find a consistent set of matched submatrices that is used to align the proteins. 

Of the pairwise alignment tools, CE, FATCAT, TM-align and DALI, we have found TM-align is the fastest while also providing high-quality pairwise structure alignments. 
It is for this reason that we use TM-align for RUPEE in top-aligned mode. 
\end{quote}
\end{reply}

\begin{point}
Did you consider DALI (http://ekhidna2.biocenter.helsinki.fi/dali/) when determining which methods you would include in your comparison?
\label{pt:rw3}
\end{point}

\begin{reply}
We added the following text at lines 92-98. The last lines provides our justification for not comparing to DALI.
\begin{quote}
DALI also provides a heuristic structure search \citep{Holm2010} of whole chains found in the PDB in addition to a tool for pairwise structure alignments using distance matrices as discussed above. 
In the case of searching, DALI first identifies matched PDB-90 representatives and then walks a pre-calculated graph of structural similarities to identify further matches in the PDB to gradually build up the set of structures similar to the query structure. 
DALI is slow in comparison to SSM, and mTM has shown better quality results than DALI \citep{Dong2018}.
\end{quote}
\end{reply}

\subsection*{Methods}

\begin{point}
The Strand and Bend/Coil parts in Fig 2 are divided into three sections, but there are only two DSSP assignment codes of these two secondary structure. The author should give an explanation with more details.
\label{pt:m1}
\end{point}

\begin{reply}
This is a misunderstanding. 
We made a number of edits at lines 139-152 to clarify this point. 
The clusters of points do not correspond to individual DSSP assignment codes. 
Each plot represents a clustering of points that corresponds to a \emph{set} of DSSP assignment codes. 
The set of DSSP assignment codes are chosen for each plot because they have similar distributions of points. 
\end{reply}

\begin{point}
The length of a shingling in this work is 3, and the length in authors’ previous work is 4. How does the length impact on the search result?
\label{pt:m2}
\end{point}

\begin{reply}
We addressed this in the previous version of this paper but we did so at the wrong location and it also lacked clarity. 
The text at lines 169-173 was moved and replaced with the following text at a more appropriate location at lines 193-198. 
\begin{quote}
In applying the Jaccard similarity to sets of shingles, the length of a shingle is chosen to balance false positives, in the case of shorter shingles, against false negatives, in the case of longer shingles.
In \citep{Ayoub2017}, we used 4 consecutive descriptors for our shingles. 
The additional requirements for structural similarity we introduce in the \emph{Operating modes} section below has the ultimate effect of reducing false positives, so this allowed us to decrease the shingle length to 3 to control better for false negatives. 
\end{quote}
\end{reply}

\begin{point}
We think that the authors should give clear definitions of ‘a run’ and the length of ‘a run’.
\label{pt:m3}
\end{point}

\begin{reply}
To address this, we added the following clause to the first sentence of the paragraph following line 182.
\begin{quote}
, where a run is a consecutive sequence of identical descriptors
\end{quote}
\end{reply}

\begin{point}
The authors choose 8000, 2000, 400 as cut-offs in search, we think that the authors should give an explanation about these cut-offs. Maybe it is better to decide the number of results based on TM-score or RMSD.
\label{pt:m4}
\end{point}

\begin{reply}
We agree that RUPEE may be improved by a detailed examination of the distribution of scores to find an ideal cut-off but we do not believe this detracts from what the paper accomplishes. This can be addressed in future work. However, we agree that some justification should be given as to how we chose the filter sizes. We added the following text at lines 276-281. Please note that the filter sizes have changed due to changes resulting from addressing \ref{pt:r7}.
\begin{quote}
The filter sizes of 8000 and 400 have been chosen based on quality of results and speed. 
We have found that increasing the size of either of these filters results in only marginal improvements in the quality of results. 
Given that performing pairwise alignments is the most time-consuming aspect of the RUPEE structure search, the marginal improvements gained from larger filter sizes have to be balanced against the number of pairwise alignments performed. 
\end{quote}
\end{reply}

\subsection*{Results}

\begin{point}
The authors use CE to get TM-score and RMSD, but mTM-align use TM-align to get TM-score and RMSD. Maybe it is unfair to RUPEE.
\label{pt:r1}
\end{point}

\begin{reply}
We no longer use CE.
We now use TM-align for pairwise alignments. 
This is addressed in \ref{pt:r7}. 
\end{reply}

\begin{point}
Scop\_d360, scop\_d62 and cath\_d99 just select domains that other methods can return more than 100 or 50 results. The sieving may eliminate some hard targets.
\label{pt:r2}
\end{point}

\begin{reply}
We added the following text at lines 306-310. 
\begin{quote}
At each point in our comparison to mTM, SSM, and CATHEDRAL, where a decision had to be made that may favor one tool over another, we were careful to make sure RUPEE is not favored. 
For instance, while filtering the benchmarks based on the number of results returned may eliminate some hard targets, we note that in all cases RUPEE returns more than 100 results. 
\end{quote}
Aside from this, we do not think calculating scoring averages with varying numbers of data present at different ranks would be fair. 
It may be the case that those missing numbers would be lower than the average if present and so calculating in their absence would unjustly give a higher scoring average for searches with missing data. 
The same would hold true for comparing precision. 
\end{reply}

\begin{point}
How diverse were your domains in scop\_d360 and scop\_d62?
\label{pt:r3}
\end{point}

\begin{reply}
We added the following sentence at line 300.
\begin{quote}
scoo\_d360 contains domains from 262 distinct folds.
\end{quote}
We added the following sentence at line 302. 
\begin{quote}
scop\_d62 contains domains from 53 distinct folds. 
\end{quote}
In addition, to further explain how we arrived at our benchmarks, we added the following sentence at lines 295-298.
\begin{quote}
To avoid introducing our own bias, we were careful to derive each benchmark from an existing benchmark used in a previously published work or an existing list of protein domains provided by an independent third party.  
\end{quote}
\end{reply}

\begin{point}
We think that the authors should compare your methods in some free-modeling targets.
\label{pt:r4}
\end{point}

\begin{reply}
There are only 31 free-modeling targets available from CASP 13, which is half of our smallest benchmark scop\_d62. 
Filtering for single segment domains, there are only 24 free-modeling targets available.
Evaluations using such a small sample size may not be meaningful. 
We also note that since RUPEE is purely geometric, its approach to free-modelling targets will not be any different than for normal targets.
\end{reply}

\begin{point}
It is unfair to compare response time with different methods based on different platforms.
\label{pt:r5}
\end{point}

\begin{reply}
We do not disagree with this point with respect to comparing `methods.'
However, we do believe the response times give a fair comparison with respect to the user experience. 
We have removed the text at lines 475-481 and replaced it with the following text at lines 442-454 at the beginning of the \emph{Response times} subsection. 
We feel the response time comparisons are important to show since they demonstrate one of the strongest features of RUPEE. 
\begin{quote}
Response times to a large degree are a measure of the amount of resources available to an application. 
Response times for RUPEE were gathered from the RUPEE web site running on a single Amazon Web Services (AWS) c5.2xlarge elastic compute (EC2) unit. 
With more resources, RUPEE response times can be further improved since pairwise alignments can be run in parallel. 
For mTM, SSM, and CATHEDRAL, we gathered response times by automating their respective web sites using the Selenium WebDriver API. 

Given that our response time comparisons are made against the respective tools running in different environments, we cannot derive solid conclusions about the efficiency of the methods themselves. 
Nevertheless, these response times do fairly compare the user experience of the respective tools. 
Moreover, in some cases the response times differ dramatically, by an order of magnitude in the case of RUPEE fast. 
\end{quote}
\end{reply}

\begin{point}
The authors should also consider regular database updates, optimally on a weekly basis following the PDB releases.
\label{pt:r6}
\end{point}

\begin{reply}
We do plan on updating RUPEE with each major update of SCOPe, CATH and ECOD. 
Aside from this, our resources are currently limited. 
With increased interest in RUPEE, that may change. 
\end{reply}

\begin{point}
In our experience, TM-align is faster than CE. If the authors use TM-align to get pairwise alignments in Top-Aligned model, maybe it would be time-saving.
\label{pt:r7}
\end{point}

\begin{reply}
This paper benefited significantly from this comment.
We have adopted the use of TM-align for pairwise alignments with RUPEE top-aligned. 
This change has resulted in changes to the paper in several locations. 
We justify our use of TM-align with the new paragraph at lines 70-72 in \emph{Related work}.
We discuss our use of TM-align with edits to the paragraph at lines 266-275 in \emph{Methods}.
Additionally, there are edits scattered throughout the entire Results section. 
Most significantly, we have added a new subsection \emph{Alignment normalization} as the first subsection of \emph{Results} in order to explain the effect of our choice of alignment normalization on the results.
\end{reply}

\subsection*{Web site}

\begin{point}
We use structure IDs (e.g. PDB ID: 8TIMA or SCOPe ID: d8TIMA\_) to search on the webserver, and there are no records.
\label{pt:ws1}
\end{point}

\begin{reply}
This was a bug that we have fixed.  
\end{reply}

\begin{point}
It would be nice to provide an input example.
\label{pt:ws2}
\end{point}

\begin{reply}
We added help text right below the `Structure Id' field. 
\end{reply}

\begin{point}
We would consider it essential to be able to download the results of the database search in a machine-readable (e.g. CSV) format.
\label{pt:ws3}
\end{point}

\begin{reply}
We added an `Export to CSV' link to the top of the results grid. 
\end{reply}

\begin{point}
It would be nice to provide demonstrations about header in table.
\label{pt:ws4}
\end{point}

\begin{reply}
We added question icons and tooltips to the one- and two-letter column names. 
\end{reply}

\begin{point}
Occasionally, the link of CE provides an error result.
\label{pt:ws5}
\end{point}

\begin{reply}
This was a bug that we have fixed. 
\end{reply}

% Reviewer 2
\reviewersection

\begin{point}
The paper says that the LCS of matched gram sequences will be applied for the top 8000 candidates for validation, and adjust the Jaccard similarity scores accordingly. How exactly does the adjustment work?
\label{pt:r21}
\end{point}

\begin{reply}
We agree that this was not adequately explained. 
We made a number of edits at lines 251-265 to clarify this point. 
The edited paragraph is shown below. 
\begin{quote}
In fast mode, the top 8000 matches from the min-hashing and LSH initial filtering are obtained.
For each match, we adjust the Jaccard similarity to account for matched shingles that are out of sequence order. 
For this adjustment, we calculate the exact denominator in the definition of Jaccard similarity from the original shingle sequences of the structures.
As for the numerator, instead of using the total number of matched shingles, we use the length of the longest common subsequence (LCS) of the shingle sequences for each match. 
This LCS adjustment to the Jaccard similarity enforces the constraint that all valid shingles matches must appear in order along the length of each structure. 
The final step of fast mode is to sort the matches based on the adjusted Jaccard similarity scores and return the results. 
\end{quote}
\end{reply}

\begin{point}
In top-aligned mode, RUPEE performs unoptimized CE for the top 2000 matches and optimized CE alignments for the top 400 matches. What's the difference between unoptimized CE and optimized CE, and how the authors decided on the numbers of matches from fast mode for further validation?
\label{pt:r22}
\end{point}

\begin{reply}
Due to our response to \ref{pt:r7}, this comment no longer applies since we are no longer using CE. 
\end{reply}

\begin{point}
It is unclear how much the MinHash and LSH helped with the speed. RUPEE retains 8000 matches after applying these techniques. What's the reduction rate here (8000 out of how many structures)?\label{pt:r23}
\end{point}

\begin{reply}
This comment was an indicator of an overall lack of clarity in the \emph{Min-hashing and LSH} subsection. 
For this reason, we made extensive edits to lines 213-238 to clarify how and why we use min-hashing and LSH. 
To address the reduction rate, we added the following sentence at lines 248-250.
\begin{quote}
In this way, RUPEE quickly compares the query structure to every structure in the database, which in the case of ECOD is greater than 600,000 protein domains.
\end{quote}
How this reduction is accomplished is clarified by the edits made to the \emph{Min-hashing and LSH} subsection. 
\end{reply}

\begin{point}
The authors compared the score curves (Fig 4) and precision curves (Fig. 5), and claimed that RUPEE performed better than mTM (even though mTM achieved higher precision than RUPEE) in capturing structural similarities that don't align with the SCOP hierarchy. I feel this argument is a little stretchy, and perhaps the authors can use some examples to demonstrate this point.
\label{pt:r24}
\end{point}

\begin{reply}
With changes made to address \ref{pt:r7}, precision for RUPEE top-aligned has improved significantly. 
Nevertheless, this comment encouraged us to check our assumptions.
While RUPEE filters for different hierarchy levels and hierarchy level representatives are useful for exploring unexpected structural relationships across folds, this does not necessarily extend to RUPEE without filters applied since there are usually enough similar structures within the fold that can be matched. 
We made a number of edits at lines 21-32 to walk back these assertions and instead focus on the benefit of applying filters.
Additionally, in our larger rework of the \emph{Precision} section we have removed lines 426-434 and we also removed text appearing at lines 539-543 of the \emph{Conclusion}.
\end{reply}

% Use the short-hand macros for one-liners.
%\shortpoint{ Typo in line xy. }
%\shortreply{ Fixed.}

\bibliography{rebuttal}
\bibliographystyle{natbib}

\end{document}


