% LaTeX rebuttal letter example.
% 2018 fzenke.net
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{lipsum} % to generate some filler text
\usepackage{fullpage}

% import Eq and Section references from the main manuscript where needed
% \usepackage{xr}
% \externaldocument{manuscript}

% package needed for optional arguments
\usepackage{xifthen}
% define counters for reviewers and their points
\newcounter{reviewer}
\setcounter{reviewer}{0}
\newcounter{point}[reviewer]
\setcounter{point}{0}

% This refines the format of how the reviewer/point reference will appear.
\renewcommand{\thepoint}{P\,\thereviewer.\arabic{point}} 

% command declarations for reviewer points and our responses
\newcommand{\reviewersection}{\stepcounter{reviewer} \bigskip \hrule
                  \section*{Reviewer \thereviewer}}

\newenvironment{point}
   {\refstepcounter{point} \bigskip \noindent {\textbf{Reviewer~Point~\thepoint} } ---\ }
   {\par }

\newcommand{\shortpoint}[1]{\refstepcounter{point}  \bigskip \noindent 
	{\textbf{Reviewer~Point~\thepoint} } ---~#1\par }

\newenvironment{reply}
   {\medskip \noindent \begin{sf}\textbf{Reply}:\  }
   {\medskip \end{sf}}

\newcommand{\shortreply}[2][]{\medskip \noindent \begin{sf}\textbf{Reply}:\  #2
	\ifthenelse{\equal{#1}{}}{}{ \hfill \footnotesize (#1)}%
	\medskip \end{sf}}



\begin{document}

\section*{Response to the reviewers}

% Both Reviewers
We thank the reviewers for their critical assessment of our work. 

% Reviewer 1
\reviewersection

\subsection*{Related work}

\begin{point}
The authors should also briefly introduce the TM-score as a metric, because the authors use this metric later.
\label{pt:rw1}
\end{point}

\begin{reply}
We have added the following text along with citations at lines 46-52.
\begin{quote}
However, the RMSD score does not factor in the distance between unaligned residues nor does it consider the percentage of aligned residues, that is, alignment \emph{coverage}. 
RMSD scores also have some dependence on the length of the aligned proteins. 
On the other hand, the TM-score takes all residues into account and normalizes for both coverage and length of the aligned proteins. 
For this reason, TM-score is frequently used in the assessment of protein structure alignments. 
\end{quote}
\end{reply}

\begin{point}
The authors should also briefly discuss and cite other pairwise structure alignment tools like TM-align and DALI.
\label{pt:rw2}
\end{point}

\begin{reply}
We have added the following text along with citations at lines 63-72. The second paragraph is part of our reply to \ref{pt:r7} and is shown here to provide further context. 
\begin{quote}
Besides CE and FATCAT, TM-align and DALI are pairwise alignment tools also in wide use, both offering their own distinct approaches to structure alignment. 
TM-align uses a rotation matrix designed to maximize the TM-score rather than minimizing the RMSD along with dynamic programming to find the best full-length alignment. 
DALI compares intramolecular distance matrices between two proteins to find a consistent set of matched submatrices that is used to align the proteins. 

Of the pairwise alignment tools, CE, FATCAT, TM-align and DALI, we have found TM-align is the fastest while also providing high-quality pairwise structure alignments. 
It is for this reason that we use TM-align for RUPEE in top-aligned mode. 
\end{quote}
\end{reply}

\begin{point}
Did you consider DALI (http://ekhidna2.biocenter.helsinki.fi/dali/) when determining which methods you would include in your comparison?
\label{pt:rw3}
\end{point}

\begin{reply}
We have added the following text along with citations at lines 92-98. The last lines provides our justification for not comparing to DALI.
\begin{quote}
DALI also provides a heuristic structure search of whole chains found in the PDB in addition to a tool for pairwise structure alignments using distance matrices as discussed above. 
In the case of searching, DALI first identifies matched PDB-90 representatives and then walks a pre-calculated graph of structural similarities to identify further matches in the PDB to gradually build up the set of structures similar to the query structure. 
DALI is slow in comparison to SSM, and mTM has shown better quality results than DALI.
\end{quote}
\end{reply}

\subsection*{Methods}

\begin{point}
The Strand and Bend/Coil parts in Fig 2 are divided into three sections, but there are only two DSSP assignment codes of these two secondary structure. The author should give an explanation with more details.
\label{pt:m1}
\end{point}

\begin{reply}
This is a misunderstanding. 
We made a number of edits at lines 139-152 to clarify this point. 
The clusters of points do not correspond to individual DSSP assignment codes. 
Each plot represents a clustering of points that corresponds to a \emph{set} of DSSP assignment codes. 
The set of DSSP assignment codes are chosen for each plot because they have similar distributions of points. 
\end{reply}

\begin{point}
The length of a shingling in this work is 3, and the length in authors’ previous work is 4. How does the length impact on the search result?
\label{pt:m2}
\end{point}

\begin{reply}
We did address this in the original paper but we did so at the wrong location and it also lacked clarity. 
The text at lines 169-173 was moved and replaced with the following text at a more appropriate location at lines 193-198. 
\begin{quote}
In applying the Jaccard similarity to sets of shingles, the length of a shingle is chosen to balance false positives, in the case of shorter shingles, against false negatives, in the case of longer shingles.
In [Ayoub2017], we used 4 consecutive descriptors for our shingles. 
The additional requirements for structural similarity we introduce in the \emph{Operating modes} section below has the ultimate effect of reducing false positives, so this allowed us to decrease the shingle length to 3 to control better for false negatives. 
\end{quote}
\end{reply}

\begin{point}
We think that the authors should give clear definitions of ‘a run’ and the length of ‘a run’.
\label{pt:m3}
\end{point}

\begin{reply}
We added the following clause to the first sentence of the paragraph following line 182 to address this. 
\begin{quote}
, where a run is a consecutive sequence of identical descriptors
\end{quote}
\end{reply}

\begin{point}
The authors choose 8000, 2000, 400 as cut-offs in search, we think that the authors should give an explanation about these cut-offs. Maybe it is better to decide the number of results based on TM-score or RMSD.
\label{pt:m4}
\end{point}

\begin{reply}
We agree that RUPEE may be improved by a detailed examination of the distribution of scores to find an ideal cut-off but we do not believe this detracts from what the paper achieves. This can be addressed in future work. However, we agree that some justification should be given as to how we chose the filter sizes. We have added the following text at lines 276-281. Please note that the filter sizes have changed due to changes resulting from addressing \ref{pt:r7}.
\begin{quote}
The filter sizes of 8000 and 400 have been chosen based on quality of results and speed. 
We have found that increasing the size of either of these filters results in only marginal improvements in the quality of results. 
Given that performing pairwise alignments is the most time-consuming aspect of the RUPEE structure search, the marginal improvements gained from larger filter sizes have to be balanced against the number of pairwise alignments performed. 
\end{quote}
\end{reply}

\subsection*{Results}

\begin{point}
The authors use CE to get TM-score and RMSD, but mTM-align use TM-align to get TM-score and RMSD. Maybe it is unfair to RUPEE.
\label{pt:r1}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
Scop\_d360, scop\_d62 and cath\_d99 just select domains that other methods can return more than 100 or 50 results. The sieving may eliminate some hard targets.
\label{pt:r2}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
How diverse were your domains in scop\_d360 and scop\_d62?
\label{pt:r3}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
We think that the authors should compare your methods in some free-modeling targets.
\label{pt:r4}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
It is unfair to compare response time with different methods based on different platforms.
\label{pt:r5}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
The authors should also consider regular database updates, optimally on a weekly basis following the PDB releases.
\label{pt:r6}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
In our experience, TM-align is faster than CE. If the authors use TM-align to get pairwise alignments in Top-Aligned model, maybe it would be time-saving.
\label{pt:r7}
\end{point}

\begin{reply}
Our reply
\end{reply}

\subsection*{Web site}

\begin{point}
We use structure IDs (e.g. PDB ID: 8TIMA or SCOPe ID: d8TIMA\_) to search on the webserver, and there are no records.
\label{pt:ws1}
\end{point}

\begin{reply}
This was a bug that we have fixed.  
\end{reply}

\begin{point}
It would be nice to provide an input example.
\label{pt:ws2}
\end{point}

\begin{reply}
We have added help text right below the `Structure Id' field. 
\end{reply}

\begin{point}
We would consider it essential to be able to download the results of the database search in a machine-readable (e.g. CSV) format.
\label{pt:ws3}
\end{point}

\begin{reply}
We have added an `Export to CSV' link to the top of the results grid. 
\end{reply}

\begin{point}
It would be nice to provide demonstrations about header in table.
\label{pt:ws4}
\end{point}

\begin{reply}
We have added question icons and tooltips to the one- and two-letter column names. 
\end{reply}

\begin{point}
Occasionally, the link of CE provides an error result.
\label{pt:ws5}
\end{point}

\begin{reply}
This was a bug that we have fixed. 
\end{reply}

% Reviewer 2
\reviewersection

\begin{point}
The paper says that the LCS of matched gram sequences will be applied for the top 8000 candidates for validation, and adjust the Jaccard similarity scores accordingly. How exactly does the adjustment work?
\label{pt:r21}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
In top-aligned mode, RUPEE performs unoptimized CE for the top 2000 matches and optimized CE alignments for the top 400 matches. What's the difference between unoptimized CE and optimized CE, and how the authors decided on the numbers of matches from fast mode for further validation?
\label{pt:r22}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
It is unclear how much the MinHash and LSH helped with the speed. RUPEE retains 8000 matches after applying these techniques. What's the reduction rate here (8000 out of how many structures)?\label{pt:r23}
\end{point}

\begin{reply}
Our reply
\end{reply}

\begin{point}
The authors compared the score curves (Fig 4) and precision curves (Fig. 5), and claimed that RUPEE performed better than mTM (even though mTM achieved higher precision than RUPEE) in capturing structural similarities that don't align with the SCOP hierarchy. I feel this argument is a little stretchy, and perhaps the authors can use some examples to demonstrate this point.
\label{pt:r24}
\end{point}

\begin{reply}
Our reply
\end{reply}

% Use the short-hand macros for one-liners.
%\shortpoint{ Typo in line xy. }
%\shortreply{ Fixed.}

\end{document}


